{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from utils import AverageMeter, accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from utils import accuracy, save_checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbedf8ea2e0fa574"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from generate_dataset import TouchFolderLabel, CalandraLabel\n",
    "from model import MultiModalMoCo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f66fd0ddfb717e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e400b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:17:15.263806992Z",
     "start_time": "2023-09-29T13:17:15.219571877Z"
    }
   },
   "id": "889d2453"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config={\n",
    "    # \"train_dataset_name\": 'calandra_label_train',\n",
    "    \"train_dataset_name\": 'tag_train',\n",
    "    \"num_channels\": 6, # should be 6 for calandra_label and 3 for tag\n",
    "    \"epochs\": 240,\n",
    "    \"log_every_n_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 16,\n",
    "    \"data_folder\": \"/home/fotis/PycharmProjects/calandra_dataset/objects_split_object_wise/\",\n",
    "    \"momentum\": 0.99,\n",
    "    \"temperature\": 0.07,\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"nn_model\": 'resnet18',\n",
    "    \"intra_dim\": 128,\n",
    "    \"inter_dim\": 128,\n",
    "    \"weight_inter_tv\": 1,\n",
    "    \"weight_inter_vt\": 1,\n",
    "    \"weight_intra_vision\": 1,\n",
    "    \"weight_intra_tactile\": 1,\n",
    "    \"pretrained_encoder\": True,\n",
    "    \"use_wandb\": True\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82ecc33f1b9e9cd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset(config['train_dataset_name'], 2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                                           num_workers=config['num_workers'], drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dc9470762c4db43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MultiModalMoCo(n_channels=config['num_channels'], m=config['momentum'], T=config['temperature'],\n",
    "                       intra_dim=config['intra_dim'], inter_dim=config['inter_dim'], nn_model=config['nn_model'],\n",
    "                       weight_inter_tv=config['weight_inter_tv'], weight_inter_vt=config['weight_inter_vt'],\n",
    "                       weight_intra_vision=config['weight_intra_vision'], weight_intra_tactile=config['weight_intra_tactile'],\n",
    "                       pretrained_encoder=config['pretrained_encoder'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b18c80ebb8e04165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training with gpu: {device}.\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                       last_epoch=-1)\n",
    "writer = SummaryWriter()\n",
    "logging.basicConfig(filename=os.path.join(writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# Set number of training epochs\n",
    "logging.info(f\"Start MViTaC training for {config['epochs']} epochs.\")\n",
    "logging.info(f\"Training with gpu: {device}.\")\n",
    "best_acc = 0\n",
    "if config['use_wandb']:\n",
    "    wandb.init(project=\"mvitac_pretraining\", config=config)\n",
    "    # name the model\n",
    "    wandb.run.name = f\"{config['nn_model']}_lr_{config['lr']}_batch_{config['batch_size']}_epochs_{config['epochs']}\"\n",
    "    \n",
    "for epoch in range(config['epochs']):\n",
    "    loss_epoch, vis_loss_intra_epoch, tac_loss_intra_epoch, vis_tac_inter_epoch, tac_vis_inter_epoch = 0, 0, 0, 0, 0\n",
    "    pbar = tqdm(train_loader)  # Wrap train_loader with tqdm\n",
    "    for idx, values in enumerate(pbar):  # Use enumerate to get idx\n",
    "        x_vision_q, x_vision_k, x_tactile_q, x_tactile_k, label = values\n",
    "        model.train()\n",
    "        if train_dataset == 'calandra_label_train':\n",
    "            #augment the samples here\n",
    "            continue\n",
    "        # send to device\n",
    "        x_vision_q = x_vision_q.to(device, non_blocking=True)\n",
    "        x_vision_k = x_vision_k.to(device, non_blocking=True)\n",
    "\n",
    "        x_tactile_q = x_tactile_q.to(device, non_blocking=True)\n",
    "        x_tactile_k = x_tactile_k.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass to get the loss\n",
    "        loss, vis_loss_intra, tac_loss_intra, vis_tac_inter, tac_vis_inter, logits, labels = model(x_vision_q, x_vision_k, x_tactile_q, x_tactile_k)\n",
    "        loss_epoch += loss.item()\n",
    "        vis_loss_intra_epoch += vis_loss_intra.item()\n",
    "        tac_loss_intra_epoch += tac_loss_intra.item()\n",
    "        vis_tac_inter_epoch += vis_tac_inter.item()\n",
    "        tac_vis_inter_epoch += tac_vis_inter.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update pbar message\n",
    "        pbar.set_description(f\"Epoch {epoch}:\\tTrain loss: {loss_epoch.item()/ (idx + 1):.2f}\")\n",
    "    \n",
    "    if epoch % config['log_every_n_epochs'] == 0:\n",
    "        top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "        writer.add_scalar('loss', loss_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/vis_loss_intra', vis_loss_intra_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/tac_loss_intra', tac_loss_intra_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/vis_tac_inter', vis_tac_inter_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/tac_vis_inter', tac_vis_inter_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('acc/top1', top1[0], global_step=epoch)\n",
    "        writer.add_scalar('acc/top5', top5[0], global_step=epoch)\n",
    "        writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step=epoch)\n",
    "        if top1[0] > best_acc:\n",
    "            best_acc = top1[0]\n",
    "            # save both the vision and tactile models\n",
    "\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'arch': 'resnet18',\n",
    "                'state_dict_vis': model.vision_base_q.state_dict(),\n",
    "                'state_dict_tac': model.tactile_base_q.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, filename=os.path.join(writer.log_dir, 'model_{}_best_object_wise.pth'.format(epoch)))\n",
    "        # torch.save(state, f'models/calandra/model_{args.task}_{epoch}_{args.batch_size}_best_object_wise_05_t05.pth')\n",
    "        if config['use_wandb']:\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss_epoch / len(train_loader), \"vis_loss_intra\": vis_loss_intra_epoch / len(train_loader),\n",
    "                       \"tac_loss_intra\": tac_loss_intra_epoch / len(train_loader), \"vis_tac_inter\": vis_tac_inter_epoch / len(train_loader),\n",
    "                       \"tac_vis_inter\": tac_vis_inter_epoch / len(train_loader), \"top1\": top1[0], \"top5\": top5[0],\n",
    "                       \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            wandb.save('models/calandra/model_{}_best_object_wise.pth'.format(epoch))\n",
    "        \n",
    "    # warmup for the first 10 epochs\n",
    "    if epoch >= 10:\n",
    "        scheduler.step()\n",
    "    logging.debug(f\"Epoch: {epoch}\\tLoss: {loss_epoch / len(train_loader)}\\tTop1: {top1[0]}\\tTop5: {top5[0]}\")\n",
    "\n",
    "    logging.info(\"Training has finished.\")\n",
    "    # save model checkpoints\n",
    "    checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(config['epochs'])\n",
    "    save_checkpoint({\n",
    "        'epoch': config['epochs'],\n",
    "        'arch': config['nn_model'],\n",
    "        'state_dict_vis': model.vision_base_q.state_dict(),\n",
    "        'state_dict_tac': model.tactile_base_q.state_dict(),   \n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename=os.path.join(writer.log_dir, checkpoint_name))\n",
    "    logging.info(f\"Model checkpoint and metadata has been saved at {writer.log_dir}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "65a426de0c22ced2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
