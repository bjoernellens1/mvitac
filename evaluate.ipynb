{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:02:53.615575646Z",
     "start_time": "2023-10-02T15:02:52.745195508Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def adapt_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Adapts the state dictionary's key names to match the expected keys of the ResNet model.\n",
    "    \"\"\"\n",
    "    adapted_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        # Remove the prefixed numbers from the key names\n",
    "        new_key = '.'.join(k.split('.')[1:])\n",
    "        adapted_state_dict[new_key] = v\n",
    "    return adapted_state_dict\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, checkpoint_path, nn_model='resnet18', pretrained=True):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.nn_model = nn_model\n",
    "        self.rgb_encoder = self.create_resnet_encoder(3)\n",
    "        self.tactile_encoder = self.create_resnet_encoder(6)\n",
    "        \n",
    "        if pretrained:\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            \n",
    "            # Adapt the state dictionary key names\n",
    "            adapted_rgb_state_dict = adapt_state_dict(checkpoint['state_dict_vis'])\n",
    "            adapted_tactile_state_dict = adapt_state_dict(checkpoint['state_dict_tac'])\n",
    "            \n",
    "            # Load the state dict for the visual and tactile encoders\n",
    "            self.rgb_encoder.load_state_dict(adapted_rgb_state_dict, strict=False)\n",
    "            self.tactile_encoder.load_state_dict(adapted_tactile_state_dict, strict=False)\n",
    "            \n",
    "            # Freeze the weights of the encoders\n",
    "            for param in self.rgb_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.tactile_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Assuming the output features of both encoders are of size 512 (e.g., for ResNet-18)\n",
    "        # Adjust this if the size is different\n",
    "        self.linear_layer = nn.Linear(512 * 2, num_classes)\n",
    "    \n",
    "    def create_resnet_encoder(self, n_channels):\n",
    "        \"\"\"Create a ResNet encoder based on the specified model type.\"\"\"\n",
    "        if self.nn_model == 'resnet18':\n",
    "            resnet = models.resnet18(pretrained=False)\n",
    "        elif self.nn_model == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=False)\n",
    "        if n_channels != 3:\n",
    "            resnet.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        features = list(resnet.children())[:-2]  # Exclude the avgpool and fc layers\n",
    "        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        features.append(nn.Flatten())\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, rgb_input, tactile_input):\n",
    "        rgb_features = self.rgb_encoder(rgb_input)\n",
    "        tactile_features = self.tactile_encoder(tactile_input)\n",
    "        \n",
    "        # Concatenate the features from both encoders\n",
    "        combined_features = torch.cat((rgb_features, tactile_features), dim=1)\n",
    "        \n",
    "        return self.linear_layer(combined_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:09:30.811071333Z",
     "start_time": "2023-10-02T15:09:30.675041931Z"
    }
   },
   "id": "bf497d42d748acac"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = 'runs/Oct02_16-23-44_cpsadmin-Z790-AORUS-ELITE-AX/model_6_best_object_wise.pth'\n",
    "\n",
    "linear_classifier = LinearClassifier(num_classes=10, checkpoint_path=checkpoint_path, nn_model='resnet18', pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:09:35.917437068Z",
     "start_time": "2023-10-02T15:09:35.654376157Z"
    }
   },
   "id": "5672c892089ef6d0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "batch_size = 256\n",
    "num_workers = 16\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset('calandra_label_train', 2)\n",
    "test_dataset = dataset.get_dataset('calandra_label_test', 2,)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=num_workers, drop_last=False, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  \n",
    "                                            num_workers=num_workers, drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:15:21.602381715Z",
     "start_time": "2023-10-02T15:15:21.212079194Z"
    }
   },
   "id": "f139475a1d71dfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# # plot a few testing image triplets\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torchvision\n",
    "# \n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.cpu().numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "#     \n",
    "# # get some random training images\n",
    "# rgb_image_q, rgb_image_k, stacked_gelsight_images_q, stacked_gelsight_images_k, label = next(iter(train_loader))\n",
    "# \n",
    "# # unstack the gelsight images\n",
    "# gelsightA_image_q, gelsightB_image_q = torch.chunk(stacked_gelsight_images_q, 2, dim=1)\n",
    "# \n",
    "# # show image in a grid\n",
    "# imshow(torchvision.utils.make_grid(rgb_image_q))\n",
    "# imshow(torchvision.utils.make_grid(gelsightA_image_q))\n",
    "# imshow(torchvision.utils.make_grid(gelsightB_image_q))\n",
    "# \n",
    "# # show the label\n",
    "# print(label)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:15:44.052662435Z",
     "start_time": "2023-10-02T15:15:44.049734848Z"
    }
   },
   "id": "28259d0b8fdc0692"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T15:15:46.134504109Z",
     "start_time": "2023-10-02T15:15:46.124307134Z"
    }
   },
   "id": "a793ad3eb38fe69e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import accuracy\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    top1_train_accuracy = 0\n",
    "    for counter, data in enumerate(train_loader):\n",
    "        rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "        \n",
    "        logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "        loss = criterion(logits, label)\n",
    "        top1 = accuracy(logits, label, topk=(1,))\n",
    "        top1_train_accuracy += top1[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    top1_train_accuracy /= (counter + 1)\n",
    "    top1_accuracy = 0\n",
    "    top5_accuracy = 0\n",
    "    for counter, data in enumerate(test_loader):\n",
    "        rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "        \n",
    "        rgb_image_q = rgb_image_q.to(device)\n",
    "        stacked_gelsight_images_q = stacked_gelsight_images_q.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "        \n",
    "        top1, top5 = accuracy(logits, label, topk=(1,5))\n",
    "        top1_accuracy += top1[0]\n",
    "        top5_accuracy += top5[0]\n",
    "    \n",
    "    top1_accuracy /= (counter + 1)\n",
    "    top5_accuracy /= (counter + 1)\n",
    "    print(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy.item():.2f}\\tTest Accuracy: {top1_accuracy.item():.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item():.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-02T15:15:46.637678185Z"
    }
   },
   "id": "807bad94eac2702f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0cf1785ec76f69b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
