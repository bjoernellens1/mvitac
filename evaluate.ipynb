{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 1. Library Imports\n",
    "In this section, we import the necessary libraries and packages that will be used throughout the notebook. This includes the PyTorch library for deep learning, torchvision for computer vision tasks, and other utilities.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e555caee1658085c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:16.749963176Z",
     "start_time": "2023-10-06T12:58:15.937192592Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "from tqdm import tqdm\n",
    "from utils import accuracy\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model Definition and Helper Functions\n",
    "\n",
    "Here, we define helper functions and the main model architecture. The `adapt_state_dict` function adapts the state dictionary for loading pretrained models. The `LinearClassifier` class is the main model that uses ResNet-based encoders for both RGB and tactile inputs. Pretrained weights can be loaded into this model for transfer learning.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12cd198500d6116c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def adapt_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Adapts the state dictionary's key names to match the expected keys of the ResNet model.\n",
    "    \"\"\"\n",
    "    adapted_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        # Remove the prefixed numbers from the key names\n",
    "        new_key = '.'.join(k.split('.')[1:])\n",
    "        adapted_state_dict[new_key] = v\n",
    "    return adapted_state_dict\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, checkpoint_path, nn_model='resnet18', pretrained=True):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.nn_model = nn_model\n",
    "        self.rgb_encoder = self.create_resnet_encoder(3)\n",
    "        self.tactile_encoder = self.create_resnet_encoder(6)\n",
    "        \n",
    "        if pretrained:\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            \n",
    "            # Adapt the state dictionary key names\n",
    "            adapted_rgb_state_dict = adapt_state_dict(checkpoint['state_dict_vis'])\n",
    "            adapted_tactile_state_dict = adapt_state_dict(checkpoint['state_dict_tac'])\n",
    "            \n",
    "            # Load the state dict for the visual and tactile encoders\n",
    "            self.rgb_encoder.load_state_dict(adapted_rgb_state_dict, strict=False)\n",
    "            self.tactile_encoder.load_state_dict(adapted_tactile_state_dict, strict=False)\n",
    "            \n",
    "            # Freeze the weights of the encoders\n",
    "            for param in self.rgb_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.tactile_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Assuming the output features of both encoders are of size 512 (e.g., for ResNet-18)\n",
    "        # Adjust this if the size is different\n",
    "        self.linear_layer = nn.Linear(512 * 2, num_classes)\n",
    "    \n",
    "    def create_resnet_encoder(self, n_channels):\n",
    "        \"\"\"Create a ResNet encoder based on the specified model type.\"\"\"\n",
    "        if self.nn_model == 'resnet18':\n",
    "            resnet = models.resnet18(pretrained=False)\n",
    "        elif self.nn_model == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=False)\n",
    "        if n_channels != 3:\n",
    "            resnet.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        features = list(resnet.children())[:-2]  # Exclude the avgpool and fc layers\n",
    "        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        features.append(nn.Flatten())\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, rgb_input, tactile_input):\n",
    "        rgb_features = self.rgb_encoder(rgb_input)\n",
    "        tactile_features = self.tactile_encoder(tactile_input)\n",
    "        \n",
    "        # Concatenate the features from both encoders\n",
    "        combined_features = torch.cat((rgb_features, tactile_features), dim=1)\n",
    "        \n",
    "        return self.linear_layer(combined_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:16.754582172Z",
     "start_time": "2023-10-06T12:58:16.753239429Z"
    }
   },
   "id": "bf497d42d748acac"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = 'runs/Oct04_09-28-47_cpsadmin-Z790-AORUS-ELITE-AX/model_700_best_object_wise.pth'\n",
    "print(f\"Using device: {device}\")\n",
    "linear_classifier = LinearClassifier(num_classes=10, checkpoint_path=checkpoint_path, nn_model='resnet18', pretrained=True)\n",
    "linear_classifier = linear_classifier.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:17.439277278Z",
     "start_time": "2023-10-06T12:58:16.755000412Z"
    }
   },
   "id": "5672c892089ef6d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Dataset Preparation\n",
    "This section prepares the datasets for training and evaluation. Data augmentation techniques might be applied to increase the variability in the training data. We also define the data loaders, which will provide batches of data during training and evaluation.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20ccc952ae798b34"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 8\n",
    "use_wandb = True\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset('calandra_label_train', 2)\n",
    "test_dataset = dataset.get_dataset('calandra_label_test', 2,)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=num_workers, drop_last=False, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  \n",
    "                                            num_workers=num_workers, drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:17.830102039Z",
     "start_time": "2023-10-06T12:58:17.440764240Z"
    }
   },
   "id": "f139475a1d71dfb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Optimizer and Loss Function\n",
    "Before training, we need to define how the model will be optimized and what loss function will measure the model's performance. Here, we initialize the Adam optimizer and the cross-entropy loss function.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17cfc93b78ffea5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:17.833307251Z",
     "start_time": "2023-10-06T12:58:17.832746660Z"
    }
   },
   "id": "a793ad3eb38fe69e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training and Evaluation Loop\n",
    "The core of the notebook. In this section, we train the model on the training data and evaluate its performance on the test data. The model's weights are updated in each epoch based on the optimizer and loss function. After training, we evaluate the model's performance on unseen data to gauge its generalization capability.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3be08fccb72298b1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mligerfotis\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/fotis/PycharmProjects/mvitac/wandb/run-20231006_145818-qv8wxisz</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/qv8wxisz' target=\"_blank\">resnet18_pretrained</a></strong> to <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/qv8wxisz' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/qv8wxisz</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 20\n",
    "if use_wandb:\n",
    "    # init wandb\n",
    "    wandb.init(project=\"calandra_object_wise_linear_classifier\", name=\"resnet18_pretrained\")\n",
    "    if not os.path.exists(f\"runs/{wandb.run.name}\"):\n",
    "        os.makedirs(f\"runs/{wandb.run.name}\")\n",
    "    subfolder = wandb.run.name\n",
    "else:\n",
    "    subfolder = \"test\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T12:58:19.534200075Z",
     "start_time": "2023-10-06T12:58:17.833910456Z"
    }
   },
   "id": "3d18ca9836d6aef5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.64\tTrain Accuracy: 65.95: 100%|██████████| 37/37 [03:05<00:00,  5.02s/it]\n",
      "Epoch 0:\tTrain Accuracy: 65.95\tTest Accuracy: 55.94\tTest Top-5 Accuracy: 100.00: 100%|██████████| 2/2 [00:37<00:00, 18.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tEpoch Loss: 0.64\tTrain Accuracy: 65.95\tTest Accuracy: 55.94\tTest Top-5 Accuracy: 100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.64\tTrain Accuracy: 65.99: 100%|██████████| 37/37 [03:05<00:00,  5.01s/it]\n",
      "Epoch 1:\tTrain Accuracy: 65.99\tTest Accuracy: 55.94\tTest Top-5 Accuracy: 100.00: 100%|██████████| 2/2 [00:36<00:00, 18.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tEpoch Loss: 0.64\tTrain Accuracy: 65.99\tTest Accuracy: 55.94\tTest Top-5 Accuracy: 100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss: 0.64\tTrain Accuracy: 66.18:  86%|████████▋ | 32/37 [03:05<00:28,  5.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m linear_classifier\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      6\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(train_loader)\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m counter, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(pbar):\n\u001B[1;32m      8\u001B[0m     rgb_image_q, _, stacked_gelsight_images_q, _, label \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     10\u001B[0m     rgb_image_q \u001B[38;5;241m=\u001B[39m rgb_image_q\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1328\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1331\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m   1283\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[0;32m-> 1284\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1285\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1286\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1120\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1132\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1133\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1134\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1135\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/queue.py:180\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m--> 180\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get()\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnot_full\u001B[38;5;241m.\u001B[39mnotify()\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:324\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 324\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    326\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_train_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    top1_train_accuracy = 0\n",
    "    epoch_loss = 0\n",
    "    linear_classifier.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for counter, data in enumerate(pbar):\n",
    "        rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "        \n",
    "        rgb_image_q = rgb_image_q.to(device)\n",
    "        stacked_gelsight_images_q = stacked_gelsight_images_q.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "        loss = criterion(logits, label)\n",
    "        epoch_loss += loss.item()\n",
    "        top1 = accuracy(logits, label, topk=(1,))\n",
    "        top1_train_accuracy += top1[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # update the progress bar message\n",
    "        pbar.set_description(f\"Epoch {epoch}: Loss: {epoch_loss / (counter + 1):.2f}\\tTrain Accuracy: {top1_train_accuracy.item() / (counter + 1):.2f}\")\n",
    "    epoch_loss /= (len(train_loader))\n",
    "    top1_train_accuracy /= (len(train_loader))\n",
    "    # save the model\n",
    "    if top1_train_accuracy > best_train_accuracy:\n",
    "        torch.save(linear_classifier.state_dict(), f\"runs/{subfolder}/linear_classifier_{epoch}_best_object_wise.pth\")\n",
    "        best_train_accuracy = top1_train_accuracy\n",
    "        \n",
    "    top1_accuracy = 0\n",
    "    top5_accuracy = 0\n",
    "    linear_classifier.eval()\n",
    "    pbar = tqdm(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for counter, data in enumerate(pbar):\n",
    "            rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "            \n",
    "            rgb_image_q = rgb_image_q.to(device)\n",
    "            stacked_gelsight_images_q = stacked_gelsight_images_q.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "            \n",
    "            top1, top5 = accuracy(logits, label, topk=(1,5))\n",
    "            top1_accuracy += top1[0]\n",
    "            top5_accuracy += top5[0]\n",
    "            \n",
    "            # update the progress bar message\n",
    "            pbar.set_description(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy.item():.2f}\\tTest Accuracy: {top1_accuracy.item()/ (counter + 1):.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item()/ (counter + 1):.2f}\")\n",
    "    \n",
    "    top1_accuracy /= (len(test_loader))\n",
    "    top5_accuracy /= (len(test_loader))\n",
    "    if use_wandb:\n",
    "        wandb.log({\"train_accuracy\": top1_train_accuracy,\n",
    "                   \"test_accuracy\": top1_accuracy,\n",
    "                   \"test_top5_accuracy\": top5_accuracy,\n",
    "                   \"epoch_loss\": epoch_loss})\n",
    "    print(f\"Epoch {epoch}:\\tEpoch Loss: {epoch_loss:.2f}\\tTrain Accuracy: {top1_train_accuracy.item():.2f}\\tTest Accuracy: {top1_accuracy.item():.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item():.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T13:15:26.157325916Z",
     "start_time": "2023-10-06T13:04:55.903427540Z"
    }
   },
   "id": "f78f0be5bcaea8de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
