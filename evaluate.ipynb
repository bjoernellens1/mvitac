{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:23:54.321496409Z",
     "start_time": "2023-10-03T08:23:53.176698556Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def adapt_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Adapts the state dictionary's key names to match the expected keys of the ResNet model.\n",
    "    \"\"\"\n",
    "    adapted_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        # Remove the prefixed numbers from the key names\n",
    "        new_key = '.'.join(k.split('.')[1:])\n",
    "        adapted_state_dict[new_key] = v\n",
    "    return adapted_state_dict\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, checkpoint_path, nn_model='resnet18', pretrained=True):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.nn_model = nn_model\n",
    "        self.rgb_encoder = self.create_resnet_encoder(3)\n",
    "        self.tactile_encoder = self.create_resnet_encoder(6)\n",
    "        \n",
    "        if pretrained:\n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            \n",
    "            # Adapt the state dictionary key names\n",
    "            adapted_rgb_state_dict = adapt_state_dict(checkpoint['state_dict_vis'])\n",
    "            adapted_tactile_state_dict = adapt_state_dict(checkpoint['state_dict_tac'])\n",
    "            \n",
    "            # Load the state dict for the visual and tactile encoders\n",
    "            self.rgb_encoder.load_state_dict(adapted_rgb_state_dict, strict=False)\n",
    "            self.tactile_encoder.load_state_dict(adapted_tactile_state_dict, strict=False)\n",
    "            \n",
    "            # Freeze the weights of the encoders\n",
    "            for param in self.rgb_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.tactile_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Assuming the output features of both encoders are of size 512 (e.g., for ResNet-18)\n",
    "        # Adjust this if the size is different\n",
    "        self.linear_layer = nn.Linear(512 * 2, num_classes)\n",
    "    \n",
    "    def create_resnet_encoder(self, n_channels):\n",
    "        \"\"\"Create a ResNet encoder based on the specified model type.\"\"\"\n",
    "        if self.nn_model == 'resnet18':\n",
    "            resnet = models.resnet18(pretrained=False)\n",
    "        elif self.nn_model == 'resnet50':\n",
    "            resnet = models.resnet50(pretrained=False)\n",
    "        if n_channels != 3:\n",
    "            resnet.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        features = list(resnet.children())[:-2]  # Exclude the avgpool and fc layers\n",
    "        features.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        features.append(nn.Flatten())\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, rgb_input, tactile_input):\n",
    "        rgb_features = self.rgb_encoder(rgb_input)\n",
    "        tactile_features = self.tactile_encoder(tactile_input)\n",
    "        \n",
    "        # Concatenate the features from both encoders\n",
    "        combined_features = torch.cat((rgb_features, tactile_features), dim=1)\n",
    "        \n",
    "        return self.linear_layer(combined_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:23:54.332619321Z",
     "start_time": "2023-10-03T08:23:54.326903717Z"
    }
   },
   "id": "bf497d42d748acac"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = 'runs/Oct02_16-23-44_cpsadmin-Z790-AORUS-ELITE-AX/model_6_best_object_wise.pth'\n",
    "print(f\"Using device: {device}\")\n",
    "linear_classifier = LinearClassifier(num_classes=10, checkpoint_path=checkpoint_path, nn_model='resnet18', pretrained=True)\n",
    "linear_classifier = linear_classifier.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:23:56.331784971Z",
     "start_time": "2023-10-03T08:23:54.329308955Z"
    }
   },
   "id": "5672c892089ef6d0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset('calandra_label_train', 2)\n",
    "test_dataset = dataset.get_dataset('calandra_label_test', 2,)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=num_workers, drop_last=False, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  \n",
    "                                            num_workers=num_workers, drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:25:57.269571551Z",
     "start_time": "2023-10-03T08:25:56.660977210Z"
    }
   },
   "id": "f139475a1d71dfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # plot a few testing image triplets\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torchvision\n",
    "# \n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.cpu().numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "#     \n",
    "# # get some random training images\n",
    "# rgb_image_q, rgb_image_k, stacked_gelsight_images_q, stacked_gelsight_images_k, label = next(iter(train_loader))\n",
    "# \n",
    "# # unstack the gelsight images\n",
    "# gelsightA_image_q, gelsightB_image_q = torch.chunk(stacked_gelsight_images_q, 2, dim=1)\n",
    "# \n",
    "# # show image in a grid\n",
    "# imshow(torchvision.utils.make_grid(rgb_image_q))\n",
    "# imshow(torchvision.utils.make_grid(gelsightA_image_q))\n",
    "# imshow(torchvision.utils.make_grid(gelsightB_image_q))\n",
    "# \n",
    "# # show the label\n",
    "# print(label)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:26:00.351095943Z",
     "start_time": "2023-10-03T08:26:00.344164011Z"
    }
   },
   "id": "28259d0b8fdc0692"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:26:01.658786536Z",
     "start_time": "2023-10-03T08:26:01.653915718Z"
    }
   },
   "id": "a793ad3eb38fe69e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:6k8f0g2e) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">resnet18_pretrained</strong> at: <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/6k8f0g2e' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/6k8f0g2e</a><br/> View job at <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMzUxOTUwOQ==/version_details/v0' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMzUxOTUwOQ==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231003_102358-6k8f0g2e/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:6k8f0g2e). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112037555555945, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c35c9258ae64417586d2ebce0fd4debd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/fotis/PycharmProjects/mvitac/wandb/run-20231003_102602-254l4enb</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/254l4enb' target=\"_blank\">resnet18_pretrained</a></strong> to <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/254l4enb' target=\"_blank\">https://wandb.ai/ligerfotis/calandra_object_wise_linear_classifier/runs/254l4enb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain Accuracy: 64.03:  28%|██▊       | 21/74 [01:25<03:35,  4.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m linear_classifier\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     11\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(train_loader)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m counter, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(pbar):\n\u001B[1;32m     13\u001B[0m     rgb_image_q, _, stacked_gelsight_images_q, _, label \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     15\u001B[0m     rgb_image_q \u001B[38;5;241m=\u001B[39m rgb_image_q\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1328\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1331\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m   1283\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[0;32m-> 1284\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1285\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1286\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1120\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1132\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1133\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1134\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1135\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/queue.py:180\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m--> 180\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get()\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnot_full\u001B[38;5;241m.\u001B[39mnotify()\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:324\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 324\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    326\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import accuracy\n",
    "epochs = 20\n",
    "# init wandb\n",
    "import wandb\n",
    "wandb.init(project=\"calandra_object_wise_linear_classifier\", name=\"resnet18_pretrained\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    top1_train_accuracy = 0\n",
    "    linear_classifier.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for counter, data in enumerate(pbar):\n",
    "        rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "        \n",
    "        rgb_image_q = rgb_image_q.to(device)\n",
    "        stacked_gelsight_images_q = stacked_gelsight_images_q.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "        loss = criterion(logits, label)\n",
    "        top1 = accuracy(logits, label, topk=(1,))\n",
    "        top1_train_accuracy += top1[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # update the progress bar message\n",
    "        pbar.set_description(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy.item()/ (counter + 1):.2f}\")\n",
    "        \n",
    "    top1_train_accuracy /= (counter + 1)\n",
    "    top1_accuracy = 0\n",
    "    top5_accuracy = 0\n",
    "    linear_classifier.eval()\n",
    "    pbar = tqdm(test_loader)\n",
    "    for counter, data in enumerate(pbar):\n",
    "        rgb_image_q, _, stacked_gelsight_images_q, _, label = data\n",
    "        \n",
    "        rgb_image_q = rgb_image_q.to(device)\n",
    "        stacked_gelsight_images_q = stacked_gelsight_images_q.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        logits = linear_classifier(rgb_image_q, stacked_gelsight_images_q)\n",
    "        \n",
    "        top1, top5 = accuracy(logits, label, topk=(1,5))\n",
    "        top1_accuracy += top1[0]\n",
    "        top5_accuracy += top5[0]\n",
    "        \n",
    "        # update the progress bar message\n",
    "        pbar.set_description(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_accuracy.item()/ (counter + 1):.2f}\\tTest Accuracy: {top1_accuracy.item()/ (counter + 1):.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item()/ (counter + 1):.2f}\")\n",
    "    \n",
    "    top1_accuracy /= (counter + 1)\n",
    "    top5_accuracy /= (counter + 1)\n",
    "    wandb.log({\"train_accuracy\": top1_train_accuracy.item()/ (counter + 1),\n",
    "               \"test_accuracy\": top1_accuracy.item()/ (counter + 1),\n",
    "               \"test_top5_accuracy\": top5_accuracy.item()/ (counter + 1)})\n",
    "    print(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy.item()/ (counter + 1):.2f}\\tTest Accuracy: {top1_accuracy.item()/ (counter + 1):.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item()/ (counter + 1):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T08:27:37.205588343Z",
     "start_time": "2023-10-03T08:26:02.110616016Z"
    }
   },
   "id": "807bad94eac2702f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b283df7532113ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
