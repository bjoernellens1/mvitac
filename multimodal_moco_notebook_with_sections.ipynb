{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from utils import AverageMeter, accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from utils import accuracy, save_checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:40:21.534812210Z",
     "start_time": "2023-09-29T13:40:20.520754273Z"
    }
   },
   "id": "cbedf8ea2e0fa574"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from generate_dataset import TouchFolderLabel, CalandraLabel\n",
    "from model import MultiModalMoCo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:40:21.538155471Z",
     "start_time": "2023-09-29T13:40:21.534674660Z"
    }
   },
   "id": "2f66fd0ddfb717e8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e400b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T13:40:21.640614409Z",
     "start_time": "2023-09-29T13:40:21.640373369Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:17:15.263806992Z",
     "start_time": "2023-09-29T13:17:15.219571877Z"
    }
   },
   "id": "889d2453"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "config={\n",
    "    \"epochs\": 10,\n",
    "    \"log_every_n_epochs\": 1,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 16,\n",
    "    \"data_folder\": \"/home/fotis/PycharmProjects/calandra_dataset/objects_split_object_wise/\",\n",
    "    \"num_channels\": 6,\n",
    "    \"momentum\": 0.99,\n",
    "    \"temperature\": 0.07,\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"nn_model\": 'resnet18',\n",
    "    \"intra_dim\": 128,\n",
    "    \"inter_dim\": 128,\n",
    "    \"weight_inter_tv\": 1,\n",
    "    \"weight_inter_vt\": 1,\n",
    "    \"weight_intra_vision\": 1,\n",
    "    \"weight_intra_tactile\": 1,\n",
    "    \"pretrained_encoder\": False,\n",
    "    \"use_wandb\": True\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:02:45.846206353Z",
     "start_time": "2023-09-29T14:02:45.835187729Z"
    }
   },
   "id": "82ecc33f1b9e9cd8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset('calandra_label', 2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                                           num_workers=config['num_workers'], drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:02:46.060682471Z",
     "start_time": "2023-09-29T14:02:46.016467864Z"
    }
   },
   "id": "4dc9470762c4db43"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MultiModalMoCo(n_channels=config['num_channels'], m=config['momentum'], T=config['temperature'],\n",
    "                       intra_dim=config['intra_dim'], inter_dim=config['inter_dim'], nn_model=config['nn_model'],\n",
    "                       weight_inter_tv=config['weight_inter_tv'], weight_inter_vt=config['weight_inter_vt'],\n",
    "                       weight_intra_vision=config['weight_intra_vision'], weight_intra_tactile=config['weight_intra_tactile'],\n",
    "                       pretrained_encoder=config['pretrained_encoder'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:02:46.478952987Z",
     "start_time": "2023-09-29T14:02:46.192941979Z"
    }
   },
   "id": "b18c80ebb8e04165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gpu: cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mligerfotis\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/fotis/PycharmProjects/mvitac/wandb/run-20230929_160247-hb564amd</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ligerfotis/mvitac_pretraining/runs/hb564amd' target=\"_blank\">breezy-galaxy-1</a></strong> to <a href='https://wandb.ai/ligerfotis/mvitac_pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ligerfotis/mvitac_pretraining' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ligerfotis/mvitac_pretraining/runs/hb564amd' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining/runs/hb564amd</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training with gpu: {device}.\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                       last_epoch=-1)\n",
    "writer = SummaryWriter()\n",
    "logging.basicConfig(filename=os.path.join(writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# Set number of training epochs\n",
    "logging.info(f\"Start MViTaC training for {config['epochs']} epochs.\")\n",
    "logging.info(f\"Training with gpu: {device}.\")\n",
    "best_acc = 0\n",
    "if config['use_wandb']:\n",
    "    wandb.init(project=\"mvitac_pretraining\", config=config)\n",
    "    # name the model\n",
    "    wandb.run.name = f\"{config['nn_model']}_lr_{config['lr']}_batch_{config['batch_size']}_epochs_{config['epochs']}\"\n",
    "    \n",
    "for epoch in range(config['epochs']):\n",
    "    loss_epoch = 0\n",
    "    pbar = tqdm(train_loader)  # Wrap train_loader with tqdm\n",
    "    for idx, values in enumerate(pbar):  # Use enumerate to get idx\n",
    "        x_vision_q, x_vision_k, x_tactile_q, x_tactile_k, label = values\n",
    "        model.train()\n",
    "        \n",
    "        # send to device\n",
    "        x_vision_q = x_vision_q.to(device, non_blocking=True)\n",
    "        x_vision_k = x_vision_k.to(device, non_blocking=True)\n",
    "\n",
    "        x_tactile_q = x_tactile_q.to(device, non_blocking=True)\n",
    "        x_tactile_k = x_tactile_k.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass to get the loss\n",
    "        loss, logits, labels = model(x_vision_q, x_vision_k, x_tactile_q, x_tactile_k)\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = loss_epoch / len(train_loader)\n",
    "    if epoch % config['log_every_n_epochs'] == 0:\n",
    "        top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "        writer.add_scalar('loss', avg_loss, global_step=epoch)\n",
    "        writer.add_scalar('acc/top1', top1[0], global_step=epoch)\n",
    "        writer.add_scalar('acc/top5', top5[0], global_step=epoch)\n",
    "        writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step=epoch)\n",
    "        if top1[0] > best_acc:\n",
    "            best_acc = top1[0]\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'arch': 'resnet18',\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, filename=os.path.join(writer.log_dir, f'checkpoint_best.pth.tar'))\n",
    "        # torch.save(state, f'models/calandra/model_{args.task}_{epoch}_{args.batch_size}_best_object_wise_05_t05.pth')\n",
    "        if config['use_wandb']:\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"top1\": top1[0], \"top5\": top5[0], \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            wandb.save('models/calandra/model_{}_best_object_wise.pth'.format(epoch))\n",
    "        \n",
    "    # warmup for the first 10 epochs\n",
    "    if epoch >= 10:\n",
    "        scheduler.step()\n",
    "    logging.debug(f\"Epoch: {epoch}\\tLoss: {loss}\\tTop1 accuracy: {top1[0]}\")\n",
    "\n",
    "    logging.info(\"Training has finished.\")\n",
    "    # save model checkpoints\n",
    "    checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(config['epochs'])\n",
    "    save_checkpoint({\n",
    "        'epoch': config['epochs'],\n",
    "        'arch': config['nn_model'],\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename=os.path.join(writer.log_dir, checkpoint_name))\n",
    "    logging.info(f\"Model checkpoint and metadata has been saved at {writer.log_dir}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-29T14:02:46.484424946Z"
    }
   },
   "id": "65a426de0c22ced2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
