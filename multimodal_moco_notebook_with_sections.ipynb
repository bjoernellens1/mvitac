{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from utils import AverageMeter, accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from utils import accuracy, save_checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:25.424848467Z",
     "start_time": "2023-09-29T14:10:25.381937999Z"
    }
   },
   "id": "cbedf8ea2e0fa574"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from generate_dataset import TouchFolderLabel, CalandraLabel\n",
    "from model import MultiModalMoCo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:25.596750798Z",
     "start_time": "2023-09-29T14:10:25.550173213Z"
    }
   },
   "id": "2f66fd0ddfb717e8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e400b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:25.764407990Z",
     "start_time": "2023-09-29T14:10:25.720559664Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:17:15.263806992Z",
     "start_time": "2023-09-29T13:17:15.219571877Z"
    }
   },
   "id": "889d2453"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "config={\n",
    "    \"epochs\": 10,\n",
    "    \"log_every_n_epochs\": 1,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 16,\n",
    "    \"data_folder\": \"/home/fotis/PycharmProjects/calandra_dataset/objects_split_object_wise/\",\n",
    "    \"num_channels\": 6,\n",
    "    \"momentum\": 0.99,\n",
    "    \"temperature\": 0.07,\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"nn_model\": 'resnet18',\n",
    "    \"intra_dim\": 128,\n",
    "    \"inter_dim\": 128,\n",
    "    \"weight_inter_tv\": 1,\n",
    "    \"weight_inter_vt\": 1,\n",
    "    \"weight_intra_vision\": 1,\n",
    "    \"weight_intra_tactile\": 1,\n",
    "    \"pretrained_encoder\": False,\n",
    "    \"use_wandb\": True\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:26.093481059Z",
     "start_time": "2023-09-29T14:10:26.083516743Z"
    }
   },
   "id": "82ecc33f1b9e9cd8"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "\n",
    "dataset = ContrastiveLearningDataset(root_folder='calandra_objects_split_object_wise')\n",
    "train_dataset = dataset.get_dataset('calandra_label', 2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                                           num_workers=config['num_workers'], drop_last=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:26.336123940Z",
     "start_time": "2023-09-29T14:10:26.313019936Z"
    }
   },
   "id": "4dc9470762c4db43"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fotis/PycharmProjects/mvitac/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = MultiModalMoCo(n_channels=config['num_channels'], m=config['momentum'], T=config['temperature'],\n",
    "                       intra_dim=config['intra_dim'], inter_dim=config['inter_dim'], nn_model=config['nn_model'],\n",
    "                       weight_inter_tv=config['weight_inter_tv'], weight_inter_vt=config['weight_inter_vt'],\n",
    "                       weight_intra_vision=config['weight_intra_vision'], weight_intra_tactile=config['weight_intra_tactile'],\n",
    "                       pretrained_encoder=config['pretrained_encoder'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:27.074761606Z",
     "start_time": "2023-09-29T14:10:26.793650246Z"
    }
   },
   "id": "b18c80ebb8e04165"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gpu: cuda.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:hb564amd) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr><tr><td>top1</td><td>▄▂▂▂▁▁▁▂▂█</td></tr><tr><td>top5</td><td>▂▅▅█▅▇▁▂▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>22.12409</td></tr><tr><td>top1</td><td>1.5625</td></tr><tr><td>top5</td><td>4.34028</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">breezy-galaxy-1</strong> at: <a href='https://wandb.ai/ligerfotis/mvitac_pretraining/runs/hb564amd' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining/runs/hb564amd</a><br/> View job at <a href='https://wandb.ai/ligerfotis/mvitac_pretraining/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjQ1MjQ0Mw==/version_details/v0' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjQ1MjQ0Mw==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230929_160247-hb564amd/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:hb564amd). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112116188888245, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4018305bf1c345cc8d4ff531263ee743"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/fotis/PycharmProjects/mvitac/wandb/run-20230929_161027-o00454ax</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ligerfotis/mvitac_pretraining/runs/o00454ax' target=\"_blank\">stellar-eon-2</a></strong> to <a href='https://wandb.ai/ligerfotis/mvitac_pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ligerfotis/mvitac_pretraining' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ligerfotis/mvitac_pretraining/runs/o00454ax' target=\"_blank\">https://wandb.ai/ligerfotis/mvitac_pretraining/runs/o00454ax</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 35\u001B[0m\n\u001B[1;32m     32\u001B[0m x_tactile_k \u001B[38;5;241m=\u001B[39m x_tactile_k\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Forward pass to get the loss\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m loss, vis_loss_intra, tac_loss_intra, vis_tac_inter, tac_vis_inter, logits, labels \u001B[38;5;241m=\u001B[39m model(x_vision_q, x_vision_k, x_tactile_q, x_tactile_k)\n\u001B[1;32m     36\u001B[0m loss_epoch \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     37\u001B[0m vis_loss_intra_epoch \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m vis_loss_intra\u001B[38;5;241m.\u001B[39mitem()\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 7, got 3)"
     ]
    }
   ],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training with gpu: {device}.\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                       last_epoch=-1)\n",
    "writer = SummaryWriter()\n",
    "logging.basicConfig(filename=os.path.join(writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# Set number of training epochs\n",
    "logging.info(f\"Start MViTaC training for {config['epochs']} epochs.\")\n",
    "logging.info(f\"Training with gpu: {device}.\")\n",
    "best_acc = 0\n",
    "if config['use_wandb']:\n",
    "    wandb.init(project=\"mvitac_pretraining\", config=config)\n",
    "    # name the model\n",
    "    wandb.run.name = f\"{config['nn_model']}_lr_{config['lr']}_batch_{config['batch_size']}_epochs_{config['epochs']}\"\n",
    "    \n",
    "for epoch in range(config['epochs']):\n",
    "    loss_epoch, vis_loss_intra_epoch, tac_loss_intra_epoch, vis_tac_inter_epoch, tac_vis_inter_epoch = 0, 0, 0, 0, 0\n",
    "    pbar = tqdm(train_loader)  # Wrap train_loader with tqdm\n",
    "    for idx, values in enumerate(pbar):  # Use enumerate to get idx\n",
    "        x_vision_q, x_vision_k, x_tactile_q, x_tactile_k, label = values\n",
    "        model.train()\n",
    "        \n",
    "        # send to device\n",
    "        x_vision_q = x_vision_q.to(device, non_blocking=True)\n",
    "        x_vision_k = x_vision_k.to(device, non_blocking=True)\n",
    "\n",
    "        x_tactile_q = x_tactile_q.to(device, non_blocking=True)\n",
    "        x_tactile_k = x_tactile_k.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass to get the loss\n",
    "        loss, vis_loss_intra, tac_loss_intra, vis_tac_inter, tac_vis_inter, logits, labels = model(x_vision_q, x_vision_k, x_tactile_q, x_tactile_k)\n",
    "        loss_epoch += loss.item()\n",
    "        vis_loss_intra_epoch += vis_loss_intra.item()\n",
    "        tac_loss_intra_epoch += tac_loss_intra.item()\n",
    "        vis_tac_inter_epoch += vis_tac_inter.item()\n",
    "        tac_vis_inter_epoch += tac_vis_inter.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % config['log_every_n_epochs'] == 0:\n",
    "        top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "        writer.add_scalar('loss', loss_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/vis_loss_intra', vis_loss_intra_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/tac_loss_intra', tac_loss_intra_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/vis_tac_inter', vis_tac_inter_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('loss/tac_vis_inter', tac_vis_inter_epoch / len(train_loader), global_step=epoch)\n",
    "        writer.add_scalar('acc/top1', top1[0], global_step=epoch)\n",
    "        writer.add_scalar('acc/top5', top5[0], global_step=epoch)\n",
    "        writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step=epoch)\n",
    "        if top1[0] > best_acc:\n",
    "            best_acc = top1[0]\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'arch': 'resnet18',\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, filename=os.path.join(writer.log_dir, f'checkpoint_best.pth.tar'))\n",
    "        # torch.save(state, f'models/calandra/model_{args.task}_{epoch}_{args.batch_size}_best_object_wise_05_t05.pth')\n",
    "        if config['use_wandb']:\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss_epoch / len(train_loader), \"vis_loss_intra\": vis_loss_intra_epoch / len(train_loader),\n",
    "                       \"tac_loss_intra\": tac_loss_intra_epoch / len(train_loader), \"vis_tac_inter\": vis_tac_inter_epoch / len(train_loader),\n",
    "                       \"tac_vis_inter\": tac_vis_inter_epoch / len(train_loader), \"top1\": top1[0], \"top5\": top5[0],\n",
    "                       \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "            wandb.save('models/calandra/model_{}_best_object_wise.pth'.format(epoch))\n",
    "        \n",
    "    # warmup for the first 10 epochs\n",
    "    if epoch >= 10:\n",
    "        scheduler.step()\n",
    "    logging.debug(f\"Epoch: {epoch}\\tLoss: {loss_epoch / len(train_loader)}\\tTop1: {top1[0]}\\tTop5: {top5[0]}\")\n",
    "\n",
    "    logging.info(\"Training has finished.\")\n",
    "    # save model checkpoints\n",
    "    checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(config['epochs'])\n",
    "    save_checkpoint({\n",
    "        'epoch': config['epochs'],\n",
    "        'arch': config['nn_model'],\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename=os.path.join(writer.log_dir, checkpoint_name))\n",
    "    logging.info(f\"Model checkpoint and metadata has been saved at {writer.log_dir}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T14:10:59.889016266Z",
     "start_time": "2023-09-29T14:10:27.256845001Z"
    }
   },
   "id": "65a426de0c22ced2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6674a50a5e8fc35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
